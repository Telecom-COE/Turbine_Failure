To address class imbalance, there are two common techniques:

Class Weights: Assigning more weight to the minority class during model training to balance the influence of both classes.

SMOTE (Synthetic Minority Over-sampling Technique): Generating synthetic data points for the minority class to balance the dataset.


1. Using Class Weights in Random Forest:
You can specify class weights directly in the Random Forest classifier to deal with imbalanced classes. Setting class_weight='balanced' makes the model automatically adjust weights inversely proportional to class frequencies.

2. Using SMOTE (Synthetic Minority Over-sampling):
SMOTE can be used to generate synthetic examples for the minority class to balance the dataset. Weâ€™ll use the imblearn library for this.

NOTE: Some times Smote will not be working